# -*- coding: utf-8 -*-
"""Mini Project PMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HRP_rtiBN4Ptk5BqUJoTmybyoHiTLBHM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
from datetime import datetime

# Mengatur agar plot lebih menarik
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# 1. Muat Data
# Pastikan nama file sesuai dengan yang diunggah
file_id = r"C:\Users\dzahr\Downloads\Data-TimeSeries.xlsx - analisis.csv"
df = pd.read_csv(file_id)

# 2. Pre-processing Awal
# Mengubah kolom 'Bulan' menjadi tipe datetime dan menjadikannya index
df['Bulan'] = pd.to_datetime(df['Bulan'], format='%b-%y')
df.set_index('Bulan', inplace=True)

# Ganti nama kolom agar lebih mudah diakses
df.columns = ['X1_curah_hujan', 'X2_lama_hujan', 'Y_jumlah_kasus']

# 3. Pemeriksaan Awal
print("--- Informasi Dataset ---")
df.info()

print("\n--- 5 Baris Pertama Data ---")
print(df.head())

print("\n--- Statistik Deskriptif ---")
print(df.describe().T)

print("\n--- Pengecekan Missing Values ---")
print(df.isnull().sum())

# Tentukan batas waktu
train_end_date = '2020-12-31'
test_start_date = '2021-01-01'

# Pembagian data
df_train = df.loc[df.index <= train_end_date].copy()
df_test = df.loc[df.index >= test_start_date].copy()

print(f"Jumlah baris data Training: {len(df_train)}")
print(f"Rentang waktu Training: {df_train.index.min().strftime('%b-%Y')} - {df_train.index.max().strftime('%b-%Y')}")
print(f"Jumlah baris data Testing: {len(df_test)}")
print(f"Rentang waktu Testing: {df_test.index.min().strftime('%b-%Y')} - {df_test.index.max().strftime('%b-%Y')}")

# Visualisasi pembagian
plt.figure(figsize=(15, 6))
plt.plot(df_train.index, df_train['Y_jumlah_kasus'], label='Training Data (Jan 2009 - Des 2020)')
plt.plot(df_test.index, df_test['Y_jumlah_kasus'], label='Testing Data (Jan 2021 - Des 2022)', color='red')
plt.axvline(datetime.strptime('2020-12-31', '%Y-%m-%d'), color='gray', linestyle='--', label='Split Point (2020/12)')
plt.title('Pembagian Data Training dan Testing')
plt.xlabel('Bulan')
plt.ylabel('Jumlah Kasus (Y)')
plt.legend()
plt.show()

def create_time_series_features(df):
    """
    Membuat fitur deret waktu (Musiman, Tren, Lag) dari index datetime.
    """
    df['bulan'] = df.index.month # Fitur Musiman (Bulan)
    df['tahun'] = df.index.year  # Fitur Tren (Tahun)
    df['kuartal'] = df.index.quarter # Fitur Musiman (Kuartal)

    # Fitur Lag (nilai Y bulan sebelumnya), sangat penting untuk forecasting
    # Kita menggunakan data gabungan (df) untuk memastikan lag tersedia
    # Namun, saat training, kita hanya menggunakan data training
    df['Y_lag_1'] = df['Y_jumlah_kasus'].shift(1)
    df['Y_lag_2'] = df['Y_jumlah_kasus'].shift(2)
    df['Y_lag_3'] = df['Y_jumlah_kasus'].shift(3)

    return df

# Gabungkan data lagi untuk feature engineering yang konsisten,
# lalu pisahkan kembali
df_full = create_time_series_features(df.copy())

# Pisahkan kembali data training dan testing setelah penambahan fitur
df_train_feat = df_full.loc[df_full.index <= train_end_date].copy()
df_test_feat = df_full.loc[df_full.index >= test_start_date].copy()

# Hapus baris dengan nilai NaN yang muncul akibat fitur lag
# Biasanya hanya baris pertama (tergantung jumlah lag)
df_train_feat.dropna(inplace=True)

print("--- Data Training dengan Fitur Tambahan (Head) ---")
print(df_train_feat.head())

def mean_absolute_percentage_error(y_true, y_pred):
    """Menghitung MAPE"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    # Menghindari pembagian dengan nol dengan nilai sangat kecil (epsilon)
    epsilon = np.finfo(np.float64).eps
    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

def train_and_evaluate_xgboost(X_train, y_train, X_test, y_test, scenario_name):
    """
    Melatih model XGBoost dan mengevaluasi hasilnya.
    """
    print(f"\n======== Skenario: {scenario_name} ========")

    # Inisialisasi Model XGBoost
    # Hyperparameter sederhana untuk contoh ini
    xgb_reg = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=42,
        tree_method='hist' # Lebih cepat untuk dataset kecil-menengah
    )

    # Pelatihan Model
    xgb_reg.fit(X_train, y_train)

    # Prediksi
    y_pred = xgb_reg.predict(X_test)

    # Evaluasi
    mape = mean_absolute_percentage_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"MAPE pada Testing Data: {mape:.2f}%")
    print(f"R2 pada Testing Data: {r2:.4f}")

    # Visualisasi Hasil Prediksi
    plt.figure(figsize=(15, 6))
    plt.plot(y_train.index, y_train, label='Training Data')
    plt.plot(y_test.index, y_test, label='Actual Testing Data', color='red')
    plt.plot(y_test.index, y_pred, label='Predicted Data', color='green', linestyle='--')
    plt.title(f'Forecasting Hasil XGBoost - Skenario: {scenario_name}')
    plt.xlabel('Bulan')
    plt.ylabel('Jumlah Kasus (Y)')
    plt.legend()
    plt.show()

    return {'scenario': scenario_name, 'MAPE': mape, 'R2': r2, 'y_pred': y_pred}

# Target Variabel
y_train = df_train_feat['Y_jumlah_kasus']
y_test = df_test_feat['Y_jumlah_kasus']

# Fitur Dasar Timeseries yang akan selalu digunakan
base_features = ['bulan', 'tahun', 'kuartal', 'Y_lag_1', 'Y_lag_2', 'Y_lag_3']

results = []

# 1. Menggunakan variable dependent saja (Time Series Features)
features_scenario_1 = base_features

X_train_s1 = df_train_feat[features_scenario_1]
X_test_s1 = df_test_feat[features_scenario_1]

result_s1 = train_and_evaluate_xgboost(
    X_train_s1, y_train, X_test_s1, y_test,
    "1. Dependent Saja (Y)"
)
results.append(result_s1)

# 2. Menggunakan variable dependent + X1
features_scenario_2 = base_features + ['X1_curah_hujan']

X_train_s2 = df_train_feat[features_scenario_2]
X_test_s2 = df_test_feat[features_scenario_2]

result_s2 = train_and_evaluate_xgboost(
    X_train_s2, y_train, X_test_s2, y_test,
    "2. Dependent (Y) + X1"
)
results.append(result_s2)

# 3. Menggunakan variable dependent + X2
features_scenario_3 = base_features + ['X2_lama_hujan']

X_train_s3 = df_train_feat[features_scenario_3]
X_test_s3 = df_test_feat[features_scenario_3]

result_s3 = train_and_evaluate_xgboost(
    X_train_s3, y_train, X_test_s3, y_test,
    "3. Dependent (Y) + X2"
)
results.append(result_s3)

# 4. Menggunakan variable dependent + X1 dan X2
features_scenario_4 = base_features + ['X1_curah_hujan', 'X2_lama_hujan']

X_train_s4 = df_train_feat[features_scenario_4]
X_test_s4 = df_test_feat[features_scenario_4]

result_s4 = train_and_evaluate_xgboost(
    X_train_s4, y_train, X_test_s4, y_test,
    "4. Dependent (Y) + X1 & X2"
)
results.append(result_s4)

# 5. Rangkuman Hasil
results_df = pd.DataFrame([
    {'Skenario': r['scenario'], 'MAPE': f"{r['MAPE']:.2f}%", 'R2': f"{r['R2']:.4f}"}
    for r in results
])

print("\n\n==============================================")
print("=== RANGKUMAN PERBANDINGAN HASIL FORECASTING ===")
print("==============================================")
print(results_df.to_markdown(index=False))



from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer


# Definisikan metrik R2 sebagai scoring utama untuk Grid Search
r2_scorer = make_scorer(r2_score)

# List untuk menyimpan hasil tuning
tuning_results = []

def run_grid_search(X_train, y_train, scenario_name, params_grid, fixed_params={}):
    """
    Melakukan Grid Search dengan R2 sebagai metrik.
    """
    # Menggabungkan parameter tetap (dari tahap sebelumnya) dengan model dasar
    xgb_base = xgb.XGBRegressor(
        objective='reg:squarederror',
        random_state=42,
        **fixed_params # Memuat parameter yang sudah dikunci dari tahap sebelumnya
    )

    grid_search = GridSearchCV(
        estimator=xgb_base,
        param_grid=params_grid,
        scoring=r2_scorer,
        cv=3,  # Menggunakan 3-Fold Cross-Validation
        verbose=1,
        n_jobs=-1
    )

    # Pelatihan Grid Search
    grid_search.fit(X_train, y_train)

    print(f"\n[HASIL TUNING {scenario_name} - TAHAP INI]")
    print(f"R2 Terbaik (CV): {grid_search.best_score_:.4f}")
    print(f"Kombinasi Parameter Terbaik: {grid_search.best_params_}")

    # Mengembalikan model terbaik dan parameter terbaik
    return grid_search.best_estimator_, grid_search.best_params_

def evaluate_final_model(best_model, X_test, y_test, scenario_name, best_params):
    """
    Evaluasi model terbaik pada data testing.
    """
    y_pred = best_model.predict(X_test)

    r2_final = r2_score(y_test, y_pred)
    rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))

    # Fungsi MAPE (Mean Absolute Percentage Error) yang kita gunakan
    def mean_absolute_percentage_error(y_true, y_pred):
        y_true, y_pred = np.array(y_true), np.array(y_pred)
        epsilon = np.finfo(np.float64).eps
        return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

    mape_final = mean_absolute_percentage_error(y_test, y_pred)

    tuning_results.append({
        'Skenario': scenario_name,
        'R2 (Testing)': f"{r2_final:.4f}",
        'RMSE (Testing)': f"{rmse_final:.2f}",
        'MAPE (Testing)': f"{mape_final:.2f}% (Abaikan jika tinggi)",
        'Best Params': best_params
    })

    print(f"--- Evaluasi Final {scenario_name} ---")
    print(f"R2 (Testing): {r2_final:.4f}")
    print(f"RMSE (Testing): {rmse_final:.2f}")

    return y_pred

# Mendefinisikan fitur untuk setiap skenario
scenario_configs = {
    "1. Dependent Saja (Y)": base_features,
    "2. Dependent (Y) + X1": base_features + ['X1_curah_hujan'],
    "3. Dependent (Y) + X2": base_features + ['X2_lama_hujan'],
    "4. Dependent (Y) + X1 & X2": base_features + ['X1_curah_hujan', 'X2_lama_hujan']
}

# Hyperparameter Grid yang akan diuji
# Tahap 1: Struktur Pohon
grid_step1 = {
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 5],
    'gamma': [0, 0.2]
}

# Tahap 2: Kecepatan Belajar (Setelah Tahap 1 selesai, kita akan gunakan hasilnya)
grid_step2 = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.05, 0.1, 0.2]
}

# --- Proses Tuning untuk Setiap Skenario ---
final_predictions = {}

for name, features in scenario_configs.items():
    print(f"\n==================================================")
    print(f"=== TUNING DIMULAI: {name} ===")
    print(f"==================================================")

    X_train = df_train_feat[features]
    X_test = df_test_feat[features]

    # --- TAHAP 1: Tuning Struktur Pohon ---
    print("\n[TAHAP 1/2: Mencari Struktur Pohon Terbaik]")
    _, params_step1 = run_grid_search(
        X_train, y_train, name,
        params_grid=grid_step1
    )

    # --- TAHAP 2: Tuning Learning Rate dan Estimator ---
    print("\n[TAHAP 2/2: Mencari Kecepatan Belajar Terbaik]")
    best_model, best_params_final = run_grid_search(
        X_train, y_train, name,
        params_grid=grid_step2,
        fixed_params=params_step1 # Kunci parameter dari Tahap 1
    )

    # Gabungkan semua parameter terbaik
    final_best_params = {**params_step1, **best_params_final}

    # --- EVALUASI AKHIR ---
    y_pred = evaluate_final_model(best_model, X_test, y_test, name, final_best_params)
    final_predictions[name] = y_pred

print("\n\n==============================================")
print("=== RANGKUMAN HASIL TUNING DAN EVALUASI AKHIR ===")
print("==============================================")
results_df = pd.DataFrame(tuning_results)
print(results_df.to_markdown(index=False))



features_scenario_2 = base_features + ['X1_curah_hujan']

X_train = df_train_feat[features_scenario_2]
y_train = df_train_feat['Y_jumlah_kasus']
X_test = df_full.loc[df_full.index > train_end_date].dropna()[features_scenario_2]
y_test = df_full.loc[df_full.index > train_end_date].dropna()['Y_jumlah_kasus']

# Metrik R2 sebagai scoring
r2_scorer = make_scorer(r2_score)

# Parameter Terbaik Tahap 1 (Fixed Parameters)
fixed_params = {
    'gamma': 0.2,
    'max_depth': 3,
    'min_child_weight': 1
}

print(f"--- Inisialisasi Data Skenario 2 (Y + X1) Siap ---")
print(f"Fixed Parameters dari Tahap Awal: {fixed_params}")

# --- TAHAP 2: TUNING SUBSAMPLING ---
print("\n==================================================")
print("=== TAHAP 2/3: TUNING SUBSAMPLING & KOLOM REGULASI ===")
print("==================================================")

# Grid untuk Subsampel dan Colsample
grid_step2 = {
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Model dasar menggunakan parameter fixed dari Tahap 1,
# dan learning rate/n_estimators default yang wajar (belum final)
xgb_base_step2 = xgb.XGBRegressor(
    objective='reg:squarederror',
    random_state=42,
    n_estimators=100,
    learning_rate=0.1,
    **fixed_params # Memuat parameter yang sudah dikunci
)

grid_search_step2 = GridSearchCV(
    estimator=xgb_base_step2,
    param_grid=grid_step2,
    scoring=r2_scorer,
    cv=3,
    verbose=0,
    n_jobs=-1
)

grid_search_step2.fit(X_train, y_train)
params_step2 = grid_search_step2.best_params_

print(f"R2 Terbaik (CV) TAHAP 2: {grid_search_step2.best_score_:.4f}")
print(f"Kombinasi Parameter Terbaik TAHAP 2: {params_step2}")

# Tambahkan parameter terbaik Tahap 2 ke fixed_params
fixed_params.update(params_step2)
print(f"\nFixed Parameters Terbaru: {fixed_params}")

# --- TAHAP 3: FINALISASI LEARNING RATE & N_ESTIMATORS ---
print("\n==================================================")
print("=== TAHAP 3/3: TUNING KECEPATAN & JUMLAH POHON ===")
print("==================================================")

# Grid untuk Learning Rate dan Estimator
grid_step3 = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.05]
}

# Model dasar menggunakan semua parameter fixed
xgb_base_step3 = xgb.XGBRegressor(
    objective='reg:squarederror',
    random_state=42,
    **fixed_params # Memuat semua parameter yang sudah dikunci
)

grid_search_step3 = GridSearchCV(
    estimator=xgb_base_step3,
    param_grid=grid_step3,
    scoring=r2_scorer,
    cv=3,
    verbose=0,
    n_jobs=-1
)

grid_search_step3.fit(X_train, y_train)
params_step3 = grid_search_step3.best_params_

print(f"R2 Terbaik (CV) TAHAP 3: {grid_search_step3.best_score_:.4f}")
print(f"Kombinasi Parameter Terbaik TAHAP 3: {params_step3}")

# Gabungkan semua parameter terbaik
final_best_params = {**fixed_params, **params_step3}

# --- EVALUASI MODEL FINAL Skenario 2 ---
best_model = grid_search_step3.best_estimator_
y_pred_final = best_model.predict(X_test)

r2_tuned = r2_score(y_test, y_pred_final)
rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_final))

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    epsilon = np.finfo(np.float64).eps
    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

def symmetric_mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    return np.mean(np.abs(y_true - y_pred) / denominator) * 100

mape_tuned = mean_absolute_percentage_error(y_test, y_pred_final)
sMAPE_tuned = symmetric_mean_absolute_percentage_error(y_test, y_pred_final)

print("\n==============================================")
print("=== HASIL AKHIR MODEL Skenario 2 (TUNED PENUH) ===")
print("==============================================")
print(f"R2 (Testing) Awal (Tanpa Subsampel): 0.4783")
print(f"R2 (Testing) Final: {r2_tuned:.4f}")
print(f"RMSE (Testing) Final: {rmse_tuned:.2f}")
print(f"sMAPE (Testing) Final: {sMAPE_tuned:.2f}%")
print(f"\nParameter Terbaik Final: {final_best_params}")

# Visualisasi Hasil Final
plt.figure(figsize=(14, 6))
plt.plot(y_test.index, y_test, label='Aktual (Testing)', color='red')
plt.plot(y_test.index, y_pred_final, label='Prediksi (Tuned)', color='green', linestyle='--')
plt.title(f'Forecasting Skenario 2 (Y + X1) - R2: {r2_tuned:.4f}')
plt.xlabel('Bulan')
plt.ylabel('Jumlah Kasus (Y)')
plt.legend()
plt.show()





features_scenario_2 = base_features + ['X1_curah_hujan']

X_train = df_train_feat[features_scenario_2]
y_train = df_train_feat['Y_jumlah_kasus']
X_test = df_full.loc[df_full.index > train_end_date].dropna()[features_scenario_2]
y_test = df_full.loc[df_full.index > train_end_date].dropna()['Y_jumlah_kasus']

r2_scorer = make_scorer(r2_score)

# --- 2. Fungsi Bantuan untuk Tuning dan Evaluasi ---
def run_grid_search_final(X_train, y_train, params_grid, fixed_params={}):
    """Melakukan Grid Search untuk R2."""
    xgb_base = xgb.XGBRegressor(
        objective='reg:squarederror',
        random_state=42,
        **fixed_params
    )

    grid_search = GridSearchCV(
        estimator=xgb_base,
        param_grid=params_grid,
        scoring=r2_scorer,
        cv=3,
        verbose=0,
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)
    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_

def symmetric_mean_absolute_percentage_error(y_true, y_pred):
    """Menghitung sMAPE."""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    return np.mean(np.abs(y_true - y_pred) / denominator) * 100

# --- TAHAP REVISI AKHIR: Kedalaman, Kecepatan, Estimator, Lambda ---
print("\n==================================================")
print("=== TUNING REVISI AKHIR: FOKUS PADA KOMPLEKSITAS ===")
print("==================================================")

# Parameter yang akan diuji
grid_final = {
    'max_depth': [3, 5, 7],         # Mencoba kedalaman yang lebih bervariasi
    'learning_rate': [0.03, 0.05, 0.1],   # Kecepatan belajar yang teruji (diturunkan)
    'n_estimators': [200, 400],     # Meningkatkan jumlah pohon
    'reg_lambda': [0.1, 1, 10]      # Regularisasi L2 (untuk mengontrol bobot)
}

# Parameter yang dikunci (dibuat konservatif/netral untuk menghindari overfitting karena subsampling)
fixed_params_final = {
    'subsample': 1.0,           # NETRAL: Semua baris digunakan
    'colsample_bytree': 1.0,    # NETRAL: Semua kolom digunakan
    'min_child_weight': 1,      # Dari hasil terbaik Tahap 1
    'gamma': 0.2                # Dari hasil terbaik Tahap 1
}

# Jalankan Grid Search
best_model_final, params_final, r2_cv_final = run_grid_search_final(
    X_train, y_train,
    params_grid=grid_final,
    fixed_params=fixed_params_final
)

# Gabungkan semua parameter terbaik
final_best_params_all = {**fixed_params_final, **params_final}

print(f"\nR2 Terbaik (CV) Revisi Akhir: {r2_cv_final:.4f}")
print(f"Parameter Terbaik TAHAP FINAL: {params_final}")

# --- EVALUASI AKHIR MODEL REVISI ---
y_pred_final = best_model_final.predict(X_test)

r2_final = r2_score(y_test, y_pred_final)
rmse_final = np.sqrt(mean_squared_error(y_test, y_pred_final))
sMAPE_final = symmetric_mean_absolute_percentage_error(y_test, y_pred_final)

print("\n==============================================")
print("=== HASIL FINAL MODEL Skenario 2 (REVISI AKHIR) ===")
print("==============================================")
print(f"R2 (Testing) Awal Terbaik: 0.4783")
print(f"R2 (Testing) Revisi Akhir: {r2_final:.4f}")
print(f"RMSE (Testing) Revisi Akhir: {rmse_final:.2f}")
print(f"sMAPE (Testing) Revisi Akhir: {sMAPE_final:.2f}%")
print(f"\nParameter Terbaik Final: {final_best_params_all}")

# Visualisasi Hasil Final
plt.figure(figsize=(14, 6))
plt.plot(y_test.index, y_test, label='Aktual (Testing)', color='red')
plt.plot(y_test.index, y_pred_final, label='Prediksi (Revisi Final)', color='green', linestyle='--')
plt.title(f'Forecasting Skenario 2 (Y + X1) - R2 Final: {r2_final:.4f}')
plt.xlabel('Bulan')
plt.ylabel('Jumlah Kasus (Y)')
plt.legend()
plt.show()